[[{"l":"Whitepaper"},{"l":"Incentivized Decentralized Data Crowdsourcing"},{"l":"Summary","p":["Data is the lifeblood of AI development, powering the algorithms that drive innovation and transformation. However, the current landscape presents two major challenges: data isolation and underserved niche datasets.","Large enterprises are increasingly treating their data as proprietary assets, imposing high barriers to access and stifling collaboration. This data isolation limits the potential for innovation and creates an uneven playing field.","Collecting data from niche verticals poses a significant hurdle. Fine-tuning AI models requires diverse and specialized datasets that are often scattered and hard to obtain. This scarcity of niche data restricts the ability to develop accurate and effective AI solutions in specific domains.","At DataX, we believe that a decentralized data bank that is openly curated and accessible by the crowd can help democratize data and promote innovation.","Our platform incentivizes people to request, contribute, verify, and process high-quality datasets by leveraging tokenomics, and the individuals who supply and curate the data are the ones who profit from its future usage which drove by its very own high quality.","Through our platform, individuals and organizations can easily access and contribute high-quality datasets, allowing for comprehensive and diverse data sets that fuel breakthroughs. We empower data scientists, researchers, and innovators to fine-tune their AI models with the specific data they need, regardless of their niche vertical.","** Keywords: ** data bank, open, crowdsourced, incentivized, decentralized, high quality"]},{"l":"How it works"},{"l":"Stakeholders","p":["Note that the different roles outlined here are not mutually exclusive. Indeed, we expect that many users will play multiple roles within the system."]},{"l":"Suppliers","p":["Entities who supply/upload datasets for entry into the database. They can upload datasets ad-hoc and set pricing terms and/or fulfill the dataset requirements of curators."]},{"l":"Curators","p":["Users who place requests for datasets and allow _ Suppliers _ to bid on fulfilling those requests. They may also optionally allow verifiers to bid on said requests for verification purposes. They are responsible for signing off on the quality of said datasets prior to them being made available for use by Consumers.","Curators will need to be DOXXED since we want them to be held accountable for misusing their vetting powers."]},{"i":"verifiers-","l":"Verifiers** **","p":["Responsible for vetting the quality of the data entered by Suppliers. Verifiers are an optional add-on - curators may choose to verify the data themselves or utilize the services of a verifier. Any community members can apply to be a verifier and tag themselves with appropriate expertise."]},{"l":"Consumers","p":["Users who use datasets available in our database to train AI models, via our API."]},{"l":"Sponsors","p":["Users who put up liquid capital to pay for the cost of accessing one or more datasets."]},{"l":"Process flow","p":["The following diagram gives a brief overview of all how each of the aforementioned types of users use the platform and interact with each other:"]},{"i":"#","p":["drawing"]},{"l":"Supplier-led dataset publishing","p":["Our platform is intended to be open for suppliers to upload new datasets as and when they choose. This allows for dataset and API pricing competition between suppliers and for outdated datasets to be deprecated over time through natural market choice.","The rough process:","A supplier uploads a new dataset and describes its features via metadata","They can optionally choose to request verifiers to verify the quality of the uploaded data.","Note that automated verification algorithms built into the platform will be executed against any and all uploaded data regardless.","Once verification is complete, the supplier sets the price of accessing the data. 2. The supplier can also set the price of the dataset to be 0, in which case they do not earn anything from consumers accessing that dataset later on.","The data is now made publicly available via the platform API. 3. Any Verifiers involved in the process will receive a share of the revenue earned from consumers accessing the data. 1. If the dataset price is 0 then the verifiers do not earn anything from consumers accessing that dataset later on.","Suppliers can continue to provide data with rich metadata even when there’s no curator 4. Again, they can optionally choose to pay Verifiers to verify that the data matches the suppliers’ description.","Note: Every dataset will have metadata indicating whether it has been verified or not and by whom. Data verified by known Verifiers will be considered more trustworthy by the community and thus charge a higher premium. Thus, it’s in a supplier’s interest to assign Verifiers to verify the quality of the data that they upload."]},{"l":"Curator-led dataset publishing"},{"i":"step-1-curator---supplier-matching-auction","l":"Step 1: Curator <-> Supplier matching auction","p":["The curator creates a tender requesting data with a specific set of features. 5. They can also optionally request at least a certain no. of rows of data. 6. They can also optionally request for verifiers to apply for verification bids 2. Note that they can choose to invite known verifiers whom they’ve previously worked with to come and bid.","Suppliers can bid for the tender.","Verifiers can bid for the tender too, if verification has been requested.","Curator can choose multiple desirable bids from both suppliers and verifiers. At Least one supplier bid must be chosen.","The required amount of tokens covering all chosen bids in aggregate is taken from the Curator wallet and escrowed by the platform","For extremely niche data sets, the Curator will obviously expect to receive more costly bids. And vice versa."]},{"i":"step-2-data-entry-and-quality-control","l":"Step 2: Data entry and quality control","p":["Any given verifier can raise an issue with the data, but a majority will need to decide on a final decision.","Curators can decide to give the quorum of verifiers the final say or keep that privilege for themselves","drawing","If the Curator requests improvements: 5. The tokens stay in escrow. 6. The data remains unavailable via the platform API. 7. The Curator provides specific improvements needed, and Suppliers upload updated data accordingly for another round of verification. 8. There will be a 180-day deadline for these changes to be made. 9. If the curator fails to sign off on the data after 180 days, the sign-off will automatically be assumed. 10. If the Suppliers fail to upload new data within 180 days then the request will be considered a failure and the Curator sign-off is impossible. 4. In this case, the escrowed tokens go back to the Curator. The data will be destroyed.","If the Curator signs off then: 3. The Suppliers and Verifiers receive tokens from escrow. 3. The platform takes a fee from suppliers’ and verifiers’ earnings. 4. The full data is now available via the platform API","If the independent review confirms the initial peer review:","If the independent review finds that the initial verifiers’ reviews were unfair:","In the case of Verifiers:","None of the data is available via the API at this point, as doing so otherwise would otherwise allow a curator to build a model without yet signing off on quality. This incentivizes the curator, to be honest about the sign-off.","Secondary Verifiers will share revenue later on from API consumption requests.","Submit a dispute and put up a collateral of 25% of the rewards that they could have received had the data qualified","Supplier enters data into the database via API / upload / etc.","The collateralized tokens will be returned to suppliers","The Curator and Verifiers check the data in order to sign off on its quality. Data checking is done by viewing it through our website and utilising the tools there-in to check validity.","The dispute is considered resolved","The platform randomly selects a pool of independent verifiers from a list of qualified and trusted users on the platform","The process:","The results of the independent verification are compared to the initial verifiers’ review to determine if there were any inconsistencies or biases","The staked tokens will be taken from Supplier and equally divided among the new Verifiers.","The submitted data will be removed.","The Supplier is compensated and data will be accepted.","The verifiers are provided with the disputed data and asked to evaluate it using the same criteria as the initial verifiers","These verifiers are required to meet a certain level of expertise and experience in the relevant field","When a Supplier believes there’s a false negative from the Verifiers, they can raise a dispute:"]},{"l":"Consumer data API consumption","p":["Consumer setups up their model and funds their account with USDC.","API requests to fetch rows of data incur a cost.","API cost is split between the platform and the original curators, verifiers and suppliers of the data."]},{"l":"Sponsoring data access","p":["A sponsor selects a dataset they wish to sponsor","They add capital to the platform to cover the costs of consumers accessing that data","They tell the world about their sponsorship"]},{"l":"Pricing and Tokenomics","p":["The platform is designed to be self-sustaining. It accomplishes this through priced access to data as well as monetary incentives for users who contribute and verify high-quality datasets.","All bid/offer pricing and API fees are denominated in USDC, a US Dollar pegged stablecoin in the bockchain ecosystem. This allows for price predictability and avoids wild fluctuations due to speculative action around a non-stable token. Onboarding non-crypto individuals and institutions to USDC will also be easier to facilitate."]},{"l":"API pricing models","p":["We will allow Supplier and Curators to set different types of pricing for their datasets:","Per-row micro-transaction pricing","Pricing for bulk batch sizes (e.g 10k rows at price point A, 50k rows at B, etc).","Single price for entire dataset.","In addition, there will be a base cost for accessing any API, priced in terms of USDC / Gigabyte downloaded via the API. The exact price for this will be determined closer to our production launch once our infrastructure is established."]},{"l":"Payments","p":["Users receive payment in the form of MLD. Specifically, USDC revenue will be used to purchase MLD from a DEX, to be deposid into our staking contract on the recipient’s behalf. They may unstake at any time. They can also choose to opt out of this system and have their USDC payment sent directly to thor wallet. However, staking MLD comes with a no. of benefits as detailed below."]},{"l":"Fees","p":["The platform charges a commission on revenue earned by curators, suppliers and verifiers:","New dataset creation payouts to suppliers and verifiers:","25% of their earnings go to the platform.","API consumption payouts to suppliers, verifiers and curators:","25% of their earnings go to the platform"]},{"l":"DAO","p":["The platform will be governed by a DAO. Our governance token is called MLD (Machine Learning Data). It will have a maximum supply of 10,000,000."]},{"l":"Staking","p":["Obtaining this token and staking it in the platform entitles the staker to a share of our platform’s revenue, proportional to their stake vs the global staked amount.","Note: we need to check if this is legally possible","Specifically, the proportion of platform revenue allocated to stakers is 50%.","Stakers are also rewarded through higher participation probabilities:","Verifiers with holdings exceeding 500 MLD for longer than 90 days stand a higher chance of being chosen to verify a dataset.","This will be implemented via a verifier “credibility” score shown next to a verifier’s profile id. The score will incorporate a number of factors such as no. of previous successful verifications, amount and duration of staked MLD, etc."]},{"l":"Treasury","p":["All fees earned by the platform - denominated in USDC - will go into the platform treasury, governed by the DAO.","At certain times, the DAO may, at its discretion, vote to use a portion of treasury funds to buy and burn a percentage of the MLD circulating supply from the market to increase scarcity and further drive value to the token."]},{"l":"Tokenomics","p":["Note that MLD will only be obtainable via the token sale and subsequently through exchanges."]},{"l":"Distribution"},{"l":"Data"},{"l":"Licensing","p":["All data will be under a Creative Commons Attribution-ShareAlike license."]},{"l":"Architecture","p":["To optimize for data availability while controlling costs, datasets will be stored centrally on commercial cloud-hosting services. We will utilize serverless functions (e.g., Cloudflare Workers) to ensure low-latency access from around the globe.","Storage will be in a data lake / warehouse??","When uploading data, suppliers are required to provide rich meta data which makes data search easy"]},{"l":"Quality control","p":["Ensuring high-quality data or providing sufficient tooling for data quality measures is critical for the success of our platform. To achieve this, we will implement the following measures for data quality control:"]},{"l":"Automated Quality Control","p":["Our platform will provide automated tools and plugins to measure data completeness and validity. Automated quality control measures include:","Completeness: Check for missing, duplications, outliers, or null values","Validity: Check for formatting errors, such as inconsistent data types or invalid characters","Impact: we need a way to measure the impact of the data, especially for step 4. One potential way is to randomly select certain ML models and measure improved model performance","These automated quality control measures will all be enforced by default, be made public to all participants beforehand, and flag any data that falls below a certain threshold. Curators can reduce or demand additional automated checks to be done, suppliers will be required to correct or provide explanations for flagged data."]},{"i":"curator--verifier-review","l":"Curator / Verifier Review","p":["The peer review process will involve a team of subject matter experts who will evaluate the data based on criteria that were made public when requesting the data. Examples:","Relevance: The data is relevant to the research question or use case.","Completeness: The data is comprehensive and includes all relevant information.","Accuracy: The data is accurate and verified through external sources, if necessary.","Consistency: The data is consistent and contains no obvious discrepancies.","Validity: The data is valid and meets any required formatting standards.","The review will be conducted on the full data set, which will be made available through our platform dashboard interface but not through our API. Because data will not be downloadable via our API until it has been finalized, we will provide a system whereby stakeholders can develop and upload additional tooling for data verification within our online interface. This will give stakeholders the flexibility they need to run custom heuristics against the dataset for validity purposes, while at the same time enhancing the overall value of our ecosystem thanks to new and improved tooling."]},{"l":"Versioning","p":["We will implement version control for datasets. Specifically, there will be per-row version control. This will enable all parties who modify and curate dataset rows to track changes and rollback if necessary.","In addition, we will enable per-dataset change tracking. Once some rows have been modified and signed off as final, the system will increment the overall dataset version. This enables consumers to know when a newer version of the dataset is available as well make sure that the version they’re currently using doesn’t change due to new data."]},{"l":"Subset Tagging","p":["We will allow for subsets of a dataset to be tagged with arbitrary labels. Such tagged subsets will then be fetchable via the API. This will allow users to train models on subsets of a dataset that may have particular properties that are useful to them.","Tagging will usually be done by curators, suppliers, and verifiers on a row-by-row basis. Batch tagging of a selection of rows will also be possible."]},{"l":"FAQ"},{"i":"why-dao","l":"Why DAO?","p":["Decentralized Autonomous Organizations (DAOs) ensure that no single entity or person can make unilateral decisions about the use of data. This helps prevent data isolation and ensures that decision-making power is distributed among the community, promoting transparency and accountability."]},{"i":"why-crypto","l":"Why crypto?","p":["Cryptocurrency incentivizes global participation by providing fast and easy off-ramps for payments. This promotes a decentralized economy where anyone can contribute to the data ecosystem and be rewarded for their efforts."]},{"i":"why-centralized-storage","l":"Why centralized storage?","p":["While we strive for decentralization in decision-making, centralized storage provides the necessary infrastructure to make data accessible and usable, and cost-manageable for everyone."]}]]